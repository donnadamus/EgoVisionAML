{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of 50 NLQ queries\n",
    " â€¢ Model Used : VSLNet (Non-Shared version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Predictions to their Video_Uid\n",
    "The following blocks ares used to map each predictions to the relative video uid. Somehow, the output of the VSLNet model do not include this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define paths to data directory and files\n",
    "annotations_dir = \"data/ego4d_data\"\n",
    "predictions_dir = \"data/prediction_VSLNet_Non_Shared\"\n",
    "\n",
    "annotations_file = os.path.join(annotations_dir, \"v1\", \"annotations/nlq_val.json\")\n",
    "predictions_file = os.path.join(predictions_dir, \"predictions.json\")\n",
    "updated_predictions_file = os.path.join(predictions_dir, \"predictions_with_video_id.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions updated and saved to data/prediction_VSLNet_Non_Shared/predictions_with_video_id.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a mapping from clip_uid to video_id\n",
    "def create_clip_to_video_mapping(annotations_path):\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    clip_to_video = {}\n",
    "    for video in annotations[\"videos\"]:\n",
    "        video_id = video[\"video_uid\"]  # Extract video_id\n",
    "        for clip in video[\"clips\"]:\n",
    "            clip_id = clip[\"clip_uid\"]  # Extract clip_id\n",
    "            clip_to_video[clip_id] = video_id  # Map clip_id to video_id\n",
    "    \n",
    "    return clip_to_video\n",
    "\n",
    "# Step 2: Update predictions with video_id\n",
    "def add_video_id_to_predictions(predictions_path, clip_to_video_mapping, output_path):\n",
    "    with open(predictions_path, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    for result in predictions[\"results\"]:\n",
    "        clip_id = result[\"clip_uid\"]\n",
    "        result[\"video_id\"] = clip_to_video_mapping.get(clip_id, \"unknown\")  # Add video_id\n",
    "    \n",
    "    # Save updated predictions\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "\n",
    "clip_to_video = create_clip_to_video_mapping(annotations_file)\n",
    "# Update predictions\n",
    "add_video_id_to_predictions(predictions_file, clip_to_video, updated_predictions_file)    \n",
    "print(f\"Predictions updated and saved to {updated_predictions_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of 50 best NLQ queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE SECTIONS TAKEN BY THE OFFICIAL NLQ EVALUATION SCRIPT ###\n",
    "### SEE   \"VSLNet_Non_Shared/UTILS/eval_nlq.py\"   FOR MORE DETAILS###\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_IoU(pred, gt):\n",
    "    \"\"\"Compute the IoU given predicted and ground truth windows.\"\"\"\n",
    "    assert isinstance(pred, list) and isinstance(gt, list)\n",
    "    pred_is_list = isinstance(pred[0], list)\n",
    "    gt_is_list = isinstance(gt[0], list)\n",
    "    if not pred_is_list:\n",
    "        pred = [pred]\n",
    "    if not gt_is_list:\n",
    "        gt = [gt]\n",
    "    pred, gt = np.array(pred), np.array(gt)\n",
    "    inter_left = np.maximum(pred[:, 0, None], gt[None, :, 0])\n",
    "    inter_right = np.minimum(pred[:, 1, None], gt[None, :, 1])\n",
    "    inter = np.maximum(0.0, inter_right - inter_left)\n",
    "    union_left = np.minimum(pred[:, 0, None], gt[None, :, 0])\n",
    "    union_right = np.maximum(pred[:, 1, None], gt[None, :, 1])\n",
    "    union = np.maximum(0.0, union_right - union_left)\n",
    "    overlap = 1.0 * inter / union\n",
    "    if not gt_is_list:\n",
    "        overlap = overlap[:, 0]\n",
    "    if not pred_is_list:\n",
    "        overlap = overlap[0]\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def evaluate_nlq_performance(\n",
    "    predictions, ground_truth, thresholds, topK, per_instance=False\n",
    "):\n",
    "    \"\"\"Evalutes the performances.\"\"\"\n",
    "    gt_dict = {}\n",
    "    num_gt_queries = 0\n",
    "\n",
    "    for video_datum in ground_truth[\"videos\"]:\n",
    "        for clip_datum in video_datum[\"clips\"]:\n",
    "            clip_uid = clip_datum[\"clip_uid\"]\n",
    "            for ann_datum in clip_datum[\"annotations\"]:\n",
    "                key = (clip_uid, ann_datum[\"annotation_uid\"])\n",
    "                gt_dict[key] = ann_datum\n",
    "                num_gt_queries += len(ann_datum[\"language_queries\"])\n",
    "\n",
    "    results = [[[] for _ in topK] for _ in thresholds]\n",
    "    average_IoU = []\n",
    "    num_instances = 0\n",
    "    \n",
    "    for pred_datum in predictions:\n",
    "        key = (pred_datum[\"clip_uid\"], pred_datum[\"annotation_uid\"])\n",
    "        assert key in gt_dict, \"Instance not present!\"\n",
    "        query_id = pred_datum[\"query_idx\"]\n",
    "        gt_datum = gt_dict[key]\n",
    "        gt_query_datum = gt_datum[\"language_queries\"][query_id]\n",
    "\n",
    "        # Compute overlap and recalls.\n",
    "        overlap = compute_IoU(\n",
    "            pred_datum[\"predicted_times\"],\n",
    "            [[gt_query_datum[\"clip_start_sec\"], gt_query_datum[\"clip_end_sec\"]]],\n",
    "        )\n",
    "        average_IoU.append(np.mean(np.sort(overlap[0])[-3:]))\n",
    "        for tt, threshold in enumerate(thresholds):\n",
    "            for rr, KK in enumerate(topK):\n",
    "                results[tt][rr].append((overlap > threshold)[:KK].any())\n",
    "        num_instances += 1\n",
    "\n",
    "    mean_results = np.array(results).mean(axis=-1)\n",
    "    mIoU = np.mean(average_IoU)\n",
    "    print(f\"Evaluated: {num_instances} / {num_gt_queries} instances\")\n",
    "    if per_instance:\n",
    "        per_instance_results = {\n",
    "            \"overlap\": overlap,\n",
    "            \"average_IoU\": average_IoU,\n",
    "            \"results\": results,\n",
    "        }\n",
    "        return mean_results, mIoU, per_instance_results\n",
    "    else:\n",
    "        return mean_results, mIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we extract the average IoU (calculated over different thresholds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated: 3874 / 3875 instances\n",
      "len of IoU array: 3874\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "# Path al file delle predictions e ground truth\n",
    "predictions_file = \"data/prediction_VSLNet_Non_Shared/predictions_with_video_id.json\"\n",
    "gt_json_path = \"data/ego4d_data/v1/annotations/nlq_val.json\"\n",
    "\n",
    "# Parametri di valutazione\n",
    "thresholds = [0.3, 0.5, 0.01]\n",
    "topK = [1, 3, 5]\n",
    "\n",
    "# Caricamento dei dati\n",
    "with open(predictions_file, 'r') as pred_file:\n",
    "    predictions = json.load(pred_file)[\"results\"]\n",
    "\n",
    "with open(gt_json_path, 'r') as gt_file:\n",
    "    ground_truth = json.load(gt_file)\n",
    "\n",
    "# Valutazione\n",
    "_, _, per_instance_results = evaluate_nlq_performance(\n",
    "    predictions, ground_truth, thresholds, topK, per_instance=True\n",
    ")\n",
    "\n",
    "# create an array containing only the average IoU values\n",
    "average_IoU = np.array(per_instance_results[\"average_IoU\"])\n",
    "print(f\"Length of IoU array: {len(average_IoU)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of combined results: 3874\n",
      "Query 1: Where did I saw socket ? (IoU: 0.9869048775762254) -- (Video ID: 8698a0ea-f434-49f9-a8e4-45220e5d4b2c, Clip UID: 2237fc47-e8c9-4751-9b02-6189913b4b4d)\n",
      "Query 2: What vegetables did I chop?  (IoU: 0.9868421052631579) -- (Video ID: f082242c-309f-48a1-97fa-5c1d6bd255fb, Clip UID: 5d531ac1-010a-4e67-ba1a-96e485b14968)\n",
      "Query 3: In what location did I see a carton? (IoU: 0.9777777777777777) -- (Video ID: 0971527a-6cd3-4c82-9d94-09b3565f4505, Clip UID: 755ac5c3-b6cd-42da-ac72-c413d0be986a)\n",
      "Query 4: What did I put in the pot (IoU: 0.9740671762206361) -- (Video ID: 4ce119de-0f42-4bd1-b387-9e19643fdddc, Clip UID: e1c79556-e8af-4e26-bc4c-633100277239)\n",
      "Query 5: how many green peppers did i put minced meat in? (IoU: 0.9696969696969697) -- (Video ID: 25e093a8-86d5-47e9-b09f-5a8afef85b74, Clip UID: f3e4cdf4-73fa-489a-8be3-c9265364da52)\n",
      "Query 6: How many drawers did I open? (IoU: 0.9637688327499627) -- (Video ID: 805989f6-0696-4de2-ad9b-0f194e0ac48d, Clip UID: eaf8d34a-0e20-45d0-a288-569df047461e)\n",
      "Query 7: What tool did I use on the machine first  (IoU: 0.9561378586882882) -- (Video ID: f681f510-cd33-48e3-bc10-4a8f2a518495, Clip UID: b8654118-84a4-4167-83c9-f268cc15f7b2)\n",
      "Query 8: Where was the genre book before I picked it? (IoU: 0.9452808653267823) -- (Video ID: d340e569-12d3-42ef-a56b-a9a25c37ef95, Clip UID: 33c3f556-cf99-45ae-9d01-9051b9b1f19e)\n",
      "Query 9: What object did I first hit into the ground? (IoU: 0.9450414306163183) -- (Video ID: eb81442c-a322-49ea-b243-a39d2e288b9b, Clip UID: d22d3eaf-9350-46b9-99d0-39fb50e246c0)\n",
      "Query 10: What did I put in the dough in the mixer?  (IoU: 0.9445781454070523) -- (Video ID: 53da674a-089d-428a-a719-e322b2de002b, Clip UID: 39ec61c9-8725-47dc-8a18-f00e27b8ab2c)\n",
      "Query 11: how many slices of bread did I spread with peanut butter. (IoU: 0.941747572815534) -- (Video ID: 90a0cc01-f498-490a-bda1-01ad94db2946, Clip UID: bbd99850-bcb0-460e-8a40-056f1e618f88)\n",
      "Query 12: what color is the sofa  (IoU: 0.9398314520297177) -- (Video ID: 6b6d73b9-e92c-4698-86df-bbe687a9e95e, Clip UID: b704e90e-d433-4b13-9f78-f2194c5f3f57)\n",
      "Query 13: did I wipe the kitchen counter (IoU: 0.9378897777777777) -- (Video ID: ff6d3d52-dda5-46dd-8515-b9b772933030, Clip UID: efc190a8-45de-4ce5-b480-b722403bcec1)\n",
      "Query 14: Did I leave the car door open? (IoU: 0.9375187345552607) -- (Video ID: 1dcc108c-8bd4-42ad-b2c5-03662be62eda, Clip UID: 47c24350-cc10-4ec6-8407-24f778981a82)\n",
      "Query 15: What chopping board did I wash? (IoU: 0.9375) -- (Video ID: 9f28e782-417c-4c8b-a7ae-42fc96a0e94f, Clip UID: d7b8f461-db42-4365-9f89-83f923528293)\n",
      "Query 16: Where was the plate before I took it? (IoU: 0.9375) -- (Video ID: b737cd68-4e0d-440a-9813-a6c90080fac5, Clip UID: ec4a3ba3-eb00-4aa8-9b41-36043ece98f7)\n",
      "Query 17: Where was the egg before I picked it? (IoU: 0.9375) -- (Video ID: 86343e9e-b932-41d3-ad6f-83f2c2fe5486, Clip UID: 4ba774a8-cd2a-4889-9971-cc91f5c1afd4)\n",
      "Query 18: What drink did I take? (IoU: 0.9344947990263861) -- (Video ID: c74c1df9-bc28-4561-bc2a-28767632cb2b, Clip UID: 3a1a5a27-7ac3-4323-8345-6717c175b09b)\n",
      "Query 19: who did I talk to in  the house? (IoU: 0.9333333333333333) -- (Video ID: 38737402-19bd-4689-9e74-3af391b15feb, Clip UID: 93231c7e-1cf4-4a20-b1f8-9cc9428915b2)\n",
      "Query 20: How many plates did i take from the top shelf? (IoU: 0.9333333333333333) -- (Video ID: 404cc1c1-f7a0-4e16-9a39-b8e2d5d9ae59, Clip UID: 1c597fc1-7bd0-4325-abbc-645e3ec71866)\n",
      "Query 21: where did I last saw my wrapper? (IoU: 0.9328545882462504) -- (Video ID: eb0d2a52-678c-4e23-8d6c-3c7a4fd86098, Clip UID: c96cc4a2-0e82-4c79-b34f-c5b853cd3ab2)\n",
      "Query 22: What cloth did I wash? (IoU: 0.9308581762182798) -- (Video ID: 584f625d-15b0-49e8-bfa6-718b9cf010e0, Clip UID: 5726971c-b3cc-43ed-8071-f6ee143e417d)\n",
      "Query 23: Where was the transparent keg before I picked it? (IoU: 0.9250480000000001) -- (Video ID: 2763dbfc-8488-4264-b31f-4b12096c4c4b, Clip UID: b1bd830a-a738-4ebe-9e88-c348def0de90)\n",
      "Query 24: How many dough did I roll ?  (IoU: 0.9247543043373163) -- (Video ID: 53da674a-089d-428a-a719-e322b2de002b, Clip UID: 39ec61c9-8725-47dc-8a18-f00e27b8ab2c)\n",
      "Query 25: Where is round brush before picked? (IoU: 0.9178197841993121) -- (Video ID: 8698a0ea-f434-49f9-a8e4-45220e5d4b2c, Clip UID: 2237fc47-e8c9-4751-9b02-6189913b4b4d)\n",
      "Query 26: what did l arrange in kitchen drawer (IoU: 0.9096181818181818) -- (Video ID: 516c9abc-dbf3-4f87-b78e-db0ab9d657ea, Clip UID: 86c3b190-a57a-4b88-a3be-4d8d47c8d867)\n",
      "Query 27: where was the phone before I picked it? (IoU: 0.9067579295033978) -- (Video ID: 477feff8-a1ff-4f43-b726-273ca0fa47e9, Clip UID: 61de3e3f-8862-4d68-ab4a-2a26e81916d1)\n",
      "Query 28: What scraper did I use? (IoU: 0.9065686691450695) -- (Video ID: 73773748-14ac-40ba-9ef8-d5a70865aeea, Clip UID: f2f4fd65-681a-41ab-b6cf-c7feb025a040)\n",
      "Query 29: what did l smear on the dough (IoU: 0.9065081599999999) -- (Video ID: 53da674a-089d-428a-a719-e322b2de002b, Clip UID: 9ab5fd9f-77e4-4438-a546-a4b6ed889f1a)\n",
      "Query 30: What block of wood did I wipe? (IoU: 0.9048791081511509) -- (Video ID: b884e44c-07d8-426e-8356-1be4905c2675, Clip UID: 6b9efa86-2580-465b-89b9-877dbd18467c)\n",
      "Query 31: Where was the plier before I picked it? (IoU: 0.8992935150146052) -- (Video ID: 571265e5-c7b7-4313-9386-fa8655b23706, Clip UID: 99db2a20-ae39-4df7-8a9c-908f282cd8d6)\n",
      "Query 32: What tool did I sharpen the pencils with? (IoU: 0.8983509999999996) -- (Video ID: cd7c3bc6-24df-465c-9b63-a7961a7558de, Clip UID: c664f078-9b34-4a58-b949-180ac4bc0980)\n",
      "Query 33: What part did I screw? (IoU: 0.895801794447182) -- (Video ID: 5802f3e8-c11e-4403-b6c4-7504257be73c, Clip UID: 13e4de75-104a-4872-8493-2bb64480169a)\n",
      "Query 34: what did l pour in the dough mixer (IoU: 0.8932171428571423) -- (Video ID: 53da674a-089d-428a-a719-e322b2de002b, Clip UID: 9ab5fd9f-77e4-4438-a546-a4b6ed889f1a)\n",
      "Query 35: Where was the glass  before I picked it? (IoU: 0.8921420634817071) -- (Video ID: eff9a4e2-a2ab-45d4-a142-64a9aa4fc6e9, Clip UID: fa7303df-7fc3-4dc7-8e39-11a108da12e3)\n",
      "Query 36: Did I leave the fridge open? (IoU: 0.8889397333333349) -- (Video ID: 4f035fa4-f388-4645-add7-89c4a0170099, Clip UID: 9c82fb4e-b385-46a8-b829-45723b17cdba)\n",
      "Query 37: what did l put in the oven (IoU: 0.8830651666666668) -- (Video ID: 53da674a-089d-428a-a719-e322b2de002b, Clip UID: 9ab5fd9f-77e4-4438-a546-a4b6ed889f1a)\n",
      "Query 38: Did i leave the microwave open? (IoU: 0.8692261809505791) -- (Video ID: 51fc62f8-00f4-44e3-af9c-7ebb63da6c3d, Clip UID: c7accb5b-fc3c-415b-986e-59643d94cdc6)\n",
      "Query 39: In what location did I drop the stainless ? (IoU: 0.8687590035029673) -- (Video ID: f324ccbc-bef5-4d68-9722-cc99bdaaa660, Clip UID: 5e59031d-0deb-4557-a3e1-ba0ba2bb5465)\n",
      "Query 40: where is my phone? (IoU: 0.868406059821312) -- (Video ID: 3fc60e72-91ad-4320-bd07-1cf753f4a5f1, Clip UID: 740a9d13-cf77-4e7d-91b5-a3ebf6d270ec)\n",
      "Query 41: what napkin did I clean my hands with? (IoU: 0.8666004919184821) -- (Video ID: 4480f4ee-d218-41bd-8cdf-a7cbe13b61a0, Clip UID: 45d01186-d09a-408c-a00d-481a2d8d9749)\n",
      "Query 42: where was the machine before I opened the door? (IoU: 0.8649361792424543) -- (Video ID: 18a3840b-7463-43c4-9aa9-b1d8e486fa84, Clip UID: c0a634a9-3dbe-44f8-9a04-7718d2a2762e)\n",
      "Query 43: What object did I light (IoU: 0.8633325819810364) -- (Video ID: 115774b6-534d-444f-b7aa-d1b834eb0ee7, Clip UID: 58fa07ae-2992-4dc9-842a-e5a73ee3d345)\n",
      "Query 44: Did I slice the okro? (IoU: 0.8604051636901859) -- (Video ID: 4ce119de-0f42-4bd1-b387-9e19643fdddc, Clip UID: e1c79556-e8af-4e26-bc4c-633100277239)\n",
      "Query 45: Where is phone (IoU: 0.8578666666666667) -- (Video ID: 225a1ffe-3e7e-4ff8-a47e-2e80989077fe, Clip UID: 2c1724ce-f438-4d63-a699-8a7f65e3cbd9)\n",
      "Query 46: What did I pour in the frying pan? (IoU: 0.8562649460591503) -- (Video ID: 25e093a8-86d5-47e9-b09f-5a8afef85b74, Clip UID: f3e4cdf4-73fa-489a-8be3-c9265364da52)\n",
      "Query 47: Where did I put the ham? (IoU: 0.855856) -- (Video ID: 1938c632-f575-49dd-8ae0-e48dbb467920, Clip UID: 7db32b15-9ea6-4a44-a103-cc04471ce4f7)\n",
      "Query 48: how many piece of tomato did I cut? (IoU: 0.8533333333333334) -- (Video ID: 82ace2ff-4a87-4abe-ba45-3965bbc13658, Clip UID: e9becd62-c5ea-4104-9d81-781a78bb7dbe)\n",
      "Query 49: How many nuts did I pick from the black plastic material? (IoU: 0.8479497629363355) -- (Video ID: 8d84ee2b-5dcb-4311-9769-a52e7b68d9e1, Clip UID: b9cd86ff-d01f-4faa-9d2b-c53799e47273)\n",
      "Query 50: Where was the plier before I picked it? (IoU: 0.8478693333333316) -- (Video ID: 180b72fa-2102-414e-a6d5-2806e53f320c, Clip UID: 5e1a8326-b006-423e-8294-0f9559c8eb18)\n"
     ]
    }
   ],
   "source": [
    "def map_predictions_to_ground_truth(predictions, ground_truth):\n",
    "    gt_dict = {}\n",
    "    for video_datum in ground_truth[\"videos\"]:\n",
    "        for clip_datum in video_datum[\"clips\"]:\n",
    "            clip_uid = clip_datum[\"clip_uid\"]\n",
    "            for ann_datum in clip_datum[\"annotations\"]:\n",
    "                key = (clip_uid, ann_datum[\"annotation_uid\"])\n",
    "                gt_dict[key] = ann_datum\n",
    "    return gt_dict\n",
    "\n",
    "def combine_results_with_predictions(predictions, gt_dict, average_IoU):\n",
    "    detailed_results = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        clip_uid = pred[\"clip_uid\"]\n",
    "        annotation_uid = pred[\"annotation_uid\"]\n",
    "        query_idx = pred[\"query_idx\"]\n",
    "        gt_entry = gt_dict[(clip_uid, annotation_uid)]\n",
    "        query_text = gt_entry[\"language_queries\"][query_idx][\"query\"]\n",
    "        detailed_results.append({\n",
    "            \"video_id\": pred[\"video_id\"],\n",
    "            \"clip_uid\": clip_uid,\n",
    "            \"annotation_uid\": annotation_uid,\n",
    "            \"query\": query_text,\n",
    "            \"iou\": average_IoU[i],\n",
    "        })\n",
    "    return detailed_results\n",
    "\n",
    "gt_dict = map_predictions_to_ground_truth(predictions, ground_truth)\n",
    "combined_results = combine_results_with_predictions(predictions, gt_dict, average_IoU)\n",
    "print(f\"Length of combined results: {len(combined_results)}\")\n",
    "top_queries = sorted(combined_results, key=lambda x: x[\"iou\"], reverse=True)[:50]\n",
    "for i, query in enumerate(top_queries):\n",
    "    print(f\"Query {i+1}: {query['query']} (IoU: {query['iou']}) -- (Video ID: {query['video_id']}, Clip UID: {query['clip_uid']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
